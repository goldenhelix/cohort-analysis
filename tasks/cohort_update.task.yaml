name: Cohort Annotation Track Updater
description: |-
  # Update Cohort Variant Frequencies

  Updates a cohort variant frequencies track by adding new samples.

  ## Process Overview
  This task merges variant data from new VCF files into an existing cohort variant
  frequencies track. It:
  1. Reads variants from the input VCF files
  2. Filters variants based on INFO and FORMAT fields filters
  3. Merges the filtered variants with any existing variant counts
  4. Updates the cohort parameter file with the new settings

  ## Important Note
  This task should not be run directly. Instead:
  1. Use the "Build Cohort Annotation Track" workflow
  2. Select your existing cohort parameter file
  3. Select the folder containing your new VCF files

  The workflow will automatically run this task with the correct parameters.

agent_requirements:
  cpu_cores: 8
  memory_gb: 16

alternative_agent_requirements:
  Panels:
    cpu_cores: 8
    memory_gb: 16
    description: Medium number of VCFs for gene panels
  Exomes:
    cpu_cores: 16
    memory_gb: 32
    description: Large number of VCFs for exomes, shallow genomes or large panels
  Genomes:
    cpu_cores: 32
    memory_gb: 64
    description: Large number of VCFs for genomes
  Many Genomes:
    cpu_cores: 64
    memory_gb: 128
    description: Very large number of VCFs for genomes

parameters:
  - name: manifest_file
    type: file
    label: Manifest File
    help: Text file containing paths to VCF files to process
    pattern_match: ["*.txt"]
  
  - name: existing_counts
    type: file
    label: Existing Counts File
    help: Optional TSF file containing existing variant counts to update
    pattern_match: ["*.tsf"]
    optional: true

  - name: out_file
    type: file
    label: Output File Base Name
    help: The output file to create
    group: "Advanced Options"
    create: true

  - name: cohort_name
    type: string
    label: Cohort Name
    help: The name of the cohort to create
    group: "Advanced Options"

  - name: series_name
    type: string
    label: Series Name
    help: The name of the series to create
    group: "Advanced Options"

  - name: sample_name_threshold
    type: integer
    label: Sample Name Threshold
    help: The maximum number of sample names to list for rare variants
    value: 20
    group: "Advanced Options"

  - name: info_filter
    type: string
    label: Filter Expression (INFO fields)
    help: INFO field symbol names, comparisons (>, <, >=, <=, ==, !=), (and, or), operators (/ , +, - , * , ~) and values
    value: all( FILTER == "MLrejected" )
    group: "Advanced Options"
    optional: true

  - name: format_filter
    type: string
    label: Filter Expression (FORMAT fields)
    help: FORMAT field symbol names, comparisons (>, <, >=, <=, ==, !=), (and, or), operators (/ , +, - , * , ~) and values
    value: "DP > 2"
    group: "Advanced Options"
    optional: true
  

steps:
  - name: create_cohort
    description: Create the initial cohort
    type: cmd
    command: bash
    args:
      - |-
        #!/usr/bin/env bash
        set -e

        #
        # Parameters for track being created
        #
        sourceName="${cohort_name} Variant Frequencies"
        seriesName="${series_name}"
        sourceVersion=$(date -u +"%Y-%m-%d")

        # This output file reports any samples and VCF inputs that were skipped because
        # the sample name was already seen in the the existing counts file
        skipped_files_path="${out_file}_skipped_duplicates.txt"

        case "${GH_WORKSPACE_ASSEMBLY}" in
            GRCh_37*)
                coordSysId="GRCh_37_g1k,Chromosome,Homo sapiens"
                ;;
            *)
                coordSysId="GRCh_38,Chromosome,Homo sapiens"
                ;;
        esac
        echo "Workspace Assembly: ${GH_WORKSPACE_ASSEMBLY} => ${coordSysId}"

        #
        # Annotation folder containing ReferenceSequenceV2-NCBI_GRCh_38_Homo_sapiens.tsf
        # or other appropriate reference sequence source
        annotations_folder="${WORKSPACE_DIR}/AppData/Common Data/Annotations"

        # Find the most recent output file that matches the out_file pattern

        # Only set existing_counts to the most recent file if it was not provided by the user
        out_file="${out_file%.tsf}"
        if [ -z "${existing_counts}" ]; then
          existing_counts=$(ls -t "${out_file}"_*.tsf 2>/dev/null | head -n 1)
        fi

        out_file="${out_file}_$(date +%s).tsf"

        existingCounts=""
        existingCountsSamples=""
        filterExisting=""

        if [ "${existing_counts##*.}" == "tsf" ]; then

          existingCounts="$existing_counts"
          existingCountsSamples="${existing_counts}:2"
          echo "Using existing counts: ${existingCounts}"

          filterExisting=$(cat << EOF
        - FilterFilesWithSamplesTask:
            samplesFilePath: "${existingCountsSamples}"
            logFile: "${skipped_files_path}"
        EOF
        )
        fi

        #
        # Sanity check: Verify TBI index files exist for all VCF files
        #
        echo "Checking for TBI index files..."
        missing_tbi_files=()
        while IFS= read -r vcf_file; do
          # Skip empty lines
          if [ -z "$vcf_file" ]; then
            continue
          fi
          
          # Check if the file ends with .vcf.gz
          if [[ "$vcf_file" == *.vcf.gz ]]; then
            tbi_file="${vcf_file}.tbi"
            if [ ! -f "$tbi_file" ]; then
              missing_tbi_files+=("$tbi_file")
            fi
          fi
        done < "${manifest_file}"
        
        # Report missing TBI files and exit if any are found
        if [ ${#missing_tbi_files[@]} -gt 0 ]; then
          echo "Error: The following TBI index files are missing:" >&2
          for missing_file in "${missing_tbi_files[@]}"; do
            echo "  $missing_file" >&2
          done
          exit 1
        fi
        echo "All TBI index files found."

        #
        # Run merge and count
        #
        gautil_path="/opt/apiserver/gautil"

        export GOLDENHELIX_USERDATA=${WORKSPACE_DIR}/AppData
        if [ -d "$WORKSPACE_DIR/AppData/VarSeq/User Data" ]; then
          export GH_CRASH_DUMP_DIR="$WORKSPACE_DIR/AppData/VarSeq/User Data"
        fi

        # Build filterByExpr section conditionally
        filterByExprSection=""
        if [ -n "${info_filter}" ] || [ -n "${format_filter}" ]; then
          # Function to handle quoting - don't quote if value already contains quotes
          quote_if_needed() {
            local value="$1"
            if [[ "$value" == *\"* ]]; then
              echo "$value"
            else
              echo "\"$value\""
            fi
          }
          
          filterByExprSection="      - filterByExpr:"
          if [ -n "${info_filter}" ]; then
            quoted_info_filter=$(quote_if_needed "${info_filter}")
            filterByExprSection="${filterByExprSection}
                  expr: ${quoted_info_filter}"
          fi
          if [ -n "${format_filter}" ]; then
            quoted_format_filter=$(quote_if_needed "${format_filter}")
            filterByExprSection="${filterByExprSection}
                  sampleExpr: ${quoted_format_filter}"
          fi
        fi

        cat << EOF > gautil_batch_file.yaml
        ${filterExisting}
        - forEach:
            inputCount: 1
            taskList:
              - stableSourcePropTransform:
                  sourceProps:
                    - StringProp:
                        name: CombineGVCFSpanRecord 
                        value: true
              - alleleicPrimitives
        ${filterByExprSection}
              - keepFields:
                  keepSymbols:
                    - RefAlt
                    - REF
                    - ALT
                    - GT
                    - END
                    - Samples
              - fullyFlattenedMultiAllelicSplit
              - leftAlign
              - trimCommonBases
              - variantCollapsing

        - mergeVariantsTransform:
            onlyMergeMatchingRefAlts: true
            mergeDifferentRecordTypes: false
            readerWorkerThreads: 8
            readersPerFlattener: 2

        - additiveCountAlleles:
            existingCountsSource: "${existingCounts}"
            existingCountsSampleSource: "${existingCountsSamples}"
            countNoCalls: true
            sourceNamePrefix: "${sourceName}"
            outputSampleNamesThreshold: ${sample_name_threshold}

        - runTaskLists:
            taskLists:
              - SourceTaskListTask:
                  taskList:
                    - createAnnotation
                    - TsfWriterTask:
                        filePath: "${out_file}"
                        sourceMeta:
                          coordSysId: "${coordSysId}"
                          seriesName: "${seriesName}"
                          sourceVersion: "${sourceVersion}"

              - SourceTaskListTask:
                  taskList:
                    - keepFields:
                        keepSymbols:
                          - Samples
                    - TsfWriterTask:
                        filePath: "${out_file}"
                        newFile: false
                        sourceMeta:
                          coordSysId: "${coordSysId}"
                          sourceVersion: "${sourceVersion}"
        EOF

        "${gautil_path}" run  \
                --annotationFolder="${annotations_folder}" \
                --manifest "${manifest_file}" \
                -c gautil_batch_file.yaml

        "${gautil_path}" precompute "${out_file}"

        # If ${skipped_files_path} is an empty file, delete it
        if [ ! -s "${skipped_files_path}" ]; then
          rm -f "${skipped_files_path}"
        fi


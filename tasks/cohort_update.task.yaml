name: Cohort Annotation Track Updater
description: |-
  # Update Cohort Variant Frequencies

  Updates a cohort variant frequencies track by adding new samples.

  ## Process Overview
  This task merges variant data from new VCF files into an existing cohort variant
  frequencies track. It:
  1. Reads variants from the input VCF files
  2. Filters variants based on INFO and FORMAT fields filters
  3. Merges the filtered variants with any existing variant counts
  4. Updates the cohort parameter file with the new settings

  ## Important Note
  This task should not be run directly. Instead:
  1. Use the "Build Cohort Annotation Track" workflow
  2. Select your existing cohort parameter file
  3. Select the folder containing your new VCF files

  The workflow will automatically run this task with the correct parameters.

agent_requirements:
  cpu_cores: 8
  memory_gb: 16

parameters:
  - name: manifest_file
    type: file
    label: Manifest File
    help: Text file containing paths to VCF files to process
    pattern_match: ["*.txt"]
  
  - name: existing_counts
    type: file
    label: Existing Counts File
    help: Optional TSF file containing existing variant counts to update
    pattern_match: ["*.tsf"]
    optional: true

  - name: out_file
    type: file
    label: Output File Base Name
    help: The output file to create
    group: "Advanced Options"

  - name: cohort_name
    type: string
    label: Cohort Name
    help: The name of the cohort to create
    group: "Advanced Options"

  - name: series_name
    type: string
    label: Series Name
    help: The name of the series to create
    group: "Advanced Options"

  - name: sample_name_threshold
    type: integer
    label: Sample Name Threshold
    help: The maximum number of sample names to list for rare variants
    value: 20
    group: "Advanced Options"

  - name: info_filter
    type: string
    label: Filter Expression (INFO fields)
    help: INFO field symbol names, comparisons (>, <, >=, <=, ==, !=), (and, or), operators (/ , +, - , * , ~) and values
    value: "all( FILTER == \"MLrejected\" )"
    group: "Advanced Options"

  - name: format_filter
    type: string
    label: Filter Expression (FORMAT fields)
    help: FORMAT field symbol names, comparisons (>, <, >=, <=, ==, !=), (and, or), operators (/ , +, - , * , ~) and values
    value: "AF > 0.03"
    group: "Advanced Options"
  

steps:
  - name: create_cohort
    description: Create the initial cohort
    type: cmd
    command: bash
    args:
      - |-
        #!/usr/bin/env bash
        set -e

        #
        # Parameters for track being created
        #
        sourceName="${cohort_name} Variant Frequencies"
        seriesName="${series_name}"
        sourceVersion=$(date -u +"%Y-%m-%d")

        # This output file reports any samples and VCF inputs that were skipped because
        # the sample name was already seen in the the existing counts file
        skipped_files_path="${out_file}_skipped_duplicates.txt"

        case "${GH_WORKSPACE_ASSEMBLY}" in
            GRCh_37*)
                coordSysId="GRCh_37_g1k,Chromosome,Homo sapiens"
                ;;
            *)
                coordSysId="GRCh_38,Chromosome,Homo sapiens"
                ;;
        esac
        echo "Workspace Assembly: ${GH_WORKSPACE_ASSEMBLY} => ${coordSysId}"

        #
        # Annotation folder containing ReferenceSequenceV2-NCBI_GRCh_38_Homo_sapiens.tsf
        # or other appropriate reference sequence source
        annotations_folder="${WORKSPACE_DIR}/AppData/Common Data/Annotations"

        # Find the most recent output file that matches the out_file pattern

        # Only set existing_counts to the most recent file if it was not provided by the user
        out_file="${out_file%.tsf}"
        if [ -z "${existing_counts}" ]; then
          existing_counts=$(ls -t "${out_file}"_*.tsf 2>/dev/null | head -n 1)
        fi

        out_file="${out_file}_$(date +%s).tsf"

        existingCounts=""
        existingCountsSamples=""
        filterExisting=""

        if [ "${existing_counts##*.}" == "tsf" ]; then

          existingCounts="$existing_counts"
          existingCountsSamples="${existing_counts}:2"
          echo "Using existing counts: ${existingCounts}"

          filterExisting=$(cat << EOF
        - FilterFilesWithSamplesTask:
            samplesFilePath: "${existingCountsSamples}"
            logFile: "${skipped_files_path}"
        EOF
        )
        fi

        #
        # Run merge and count
        #
        gautil_path="/opt/apiserver/gautil"

        export GOLDENHELIX_USERDATA=${WORKSPACE_DIR}/AppData
        if [ -d "$WORKSPACE_DIR/AppData/VarSeq/User Data" ]; then
          export GH_CRASH_DUMP_DIR="$WORKSPACE_DIR/AppData/VarSeq/User Data"
        fi

        cat << EOF > gautil_batch_file.txt
        ${filterExisting}
        - forEach:
            inputCount: 1
            taskList:
              - alleleicPrimitives
              - filterByExpr:
                  expr: ${info_filter}
                  sampleExpr: ${format_filter}
              - keepFields:
                  keepSymbols:
                    - RefAlt
                    - REF
                    - ALT
                    - GT
                    - END
                    - Samples
              - fullyFlattenedMultiAllelicSplit
              - leftAlign
              - trimCommonBases
              - variantCollapsing

        - mergeVariantsTransform:
            onlyMergeMatchingRefAlts: true
            mergeDifferentRecordTypes: false
            readerWorkerThreads: 5
            readersPerFlattener: 10

        - additiveCountAlleles:
            existingCountsSource: "${existingCounts}"
            existingCountsSampleSource: "${existingCountsSamples}"
            countNoCalls: true
            sourceNamePrefix: "${sourceName}"
            outputSampleNamesThreshold: ${sample_name_threshold}

        - filterByExpr:
            # Only keep variants where frequency > 0
            expr: any(AlleleCounts >= 0)

        - runTaskLists:
            taskLists:
              - SourceTaskListTask:
                  taskList:
                    - createAnnotation
                    - TsfWriterTask:
                        filePath: "${out_file}"
                        sourceMeta:
                          coordSysId: "${coordSysId}"
                          seriesName: "${seriesName}"
                          sourceVersion: "${sourceVersion}"

              - SourceTaskListTask:
                  taskList:
                    - keepFields:
                        keepSymbols:
                          - Samples
                    - TsfWriterTask:
                        filePath: "${out_file}"
                        newFile: false
                        sourceMeta:
                          coordSysId: "${coordSysId}"
                          sourceVersion: "${sourceVersion}"
        EOF

        "${gautil_path}" run  \
                --annotationFolder="${annotations_folder}" \
                --manifest "${manifest_file}" \
                -c gautil_batch_file.txt

        "${gautil_path}" precompute "${out_file}"

        # If ${skipped_files_path} is an empty file, delete it
        if [ ! -s "${skipped_files_path}" ]; then
          rm -f "${skipped_files_path}"
        fi

